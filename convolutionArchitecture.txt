    +----------------------------------------+
    |            Input Image                 |
    |               (X)                      |
    +----------------------------------------+
                      |
                      V
    +----------------------------------------+
    |         Convolutional Layer            |
    |        (filters=64, kernel=3x3)        |
    |          Activation: ReLU              |
    +----------------------------------------+
                      |
                      V
    +----------------------------------------+
    |         Max-Pooling Layer              |
    |               (2x2)                    |
    +----------------------------------------+
                      |
                      V
    +----------------------------------------+
    |         Convolutional Layer            |
    |        (filters=128, kernel=3x3)       |
    |          Activation: ReLU              |
    +----------------------------------------+
                      |
                      V
    +----------------------------------------+
    |         Max-Pooling Layer              |
    |               (2x2)                    |
    +----------------------------------------+
                      |
                      V
    +----------------------------------------+
    |    Custom Conv Layer with RCSA         |
    +----------------------------------------+
    |   +------------------------------+     |
    |   | Convolutional Layer          |     |
    |   | (filters=256, kernel=3x3)    |     |
    |   | Activation: ReLU             |     |
    |   +-------------+----------------+     |
    |                 |                      |
    |                 V                      |
    |   +-------------+----------------+     |
    |   | RCSA Module                  |     |
    |   +------------------------------+     |
    |   | Residual Connections         |     |
    |   | Cross-Scale Attention        |     |
    |   +------------------------------+     |
    +----------------------------------------+
                      |
                      V
    +----------------------------------------+
    | Condensation Attention Module          |
    +----------------------------------------+
    |   +------------------------------+     |
    |   | Channel Attention            |     |
    |   +------------------------------+     |
    |   | Spatial Attention            |     |
    |   +------------------------------+     |
    +----------------------------------------+
                      |
                      V
    +----------------------------------------+
    |           Flatten Layer                |
    +----------------------------------------+
                      |
                      V
    +----------------------------------------+
    |          Fully Connected Layer         |
    |             (units=128)                |
    |          Activation: ReLU              |
    +----------------------------------------+
                      |
                      V
    +----------------------------------------+
    |          Fully Connected Layer         |
    |             (units=64)                 |
    |          Activation: ReLU              |
    +----------------------------------------+
                      |
                      V
    +----------------------------------------+
    |            Output Layer                |
    |             (units=N)                  |
    |          Activation: Softmax           |
    +----------------------------------------+
