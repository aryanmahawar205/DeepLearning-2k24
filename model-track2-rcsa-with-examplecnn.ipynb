{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8367030,"sourceType":"datasetVersion","datasetId":4973674}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n# Define the dataset path\ndataset_path = '/kaggle/input/diamos-plant-dataset/Pear/leaves'\n\n# Check if the dataset path exists\nif not os.path.exists(dataset_path):\n    print(f\"Error: Dataset path '{dataset_path}' does not exist.\")\nelse:\n    # Count the number of files in each subfolder (Pear and leaves)\n    for folder in os.listdir(dataset_path):\n        folder_path = os.path.join(dataset_path, folder)\n        if os.path.isdir(folder_path):\n            num_files = len(os.listdir(folder_path))\n            print(f\"Number of files in '{folder}' folder: {num_files}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:36:19.405275Z","iopub.execute_input":"2024-07-01T12:36:19.405618Z","iopub.status.idle":"2024-07-01T12:36:19.419932Z","shell.execute_reply.started":"2024-07-01T12:36:19.405589Z","shell.execute_reply":"2024-07-01T12:36:19.418918Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Number of files in 'curl' folder: 65\nNumber of files in 'healthy' folder: 43\nNumber of files in 'spot' folder: 1768\nNumber of files in 'slug' folder: 4050\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom PIL import Image, UnidentifiedImageError\nfrom tqdm import tqdm\n\n# Define the Channel Shuffle class\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n\n    def forward(self, x):\n        batchsize, num_channels, height, width = x.data.size()\n        channels_per_group = num_channels // self.groups\n        # Reshape\n        x = x.view(batchsize, self.groups, channels_per_group, height, width)\n        # Transpose\n        x = torch.transpose(x, 1, 2).contiguous()\n        # Flatten\n        x = x.view(batchsize, -1, height, width)\n        return x\n\n# Define the RCSA module\nclass RCSA(nn.Module):\n    def __init__(self, in_channels, reduction=16, groups=4):\n        super(RCSA, self).__init__()\n        self.groups = groups\n        self.channel_shuffle = ChannelShuffle(groups)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv1 = nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        residual = x\n        # Channel Shuffle\n        x = self.channel_shuffle(x)\n        # Squeeze and Excitation\n        x = self.avg_pool(x)\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.sigmoid(x)\n        # Scale\n        x = residual * x\n        return x\n\n# Define the CNN with RCSA\nclass ExampleCNN(nn.Module):\n    def __init__(self, num_classes=10):\n        super(ExampleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.rcsa1 = RCSA(64)\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.rcsa2 = RCSA(128)\n        self.fc = nn.Linear(128 * 8 * 8, num_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.rcsa1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.rcsa2(x)\n        x = F.relu(x)\n        x = F.adaptive_avg_pool2d(x, (8, 8))\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        return x\n\n# Custom dataset class to check if images are valid\nclass ValidImageFolder(Dataset):\n    def __init__(self, root, transform=None):\n        self.root = root\n        self.transform = transform\n        self.classes, self.class_to_idx = self._find_classes(root)\n        self.samples = self.make_dataset(root, self.class_to_idx)\n\n    def _find_classes(self, dir):\n        classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n        classes.sort()\n        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n        return classes, class_to_idx\n\n    def make_dataset(self, directory, class_to_idx):\n        instances = []\n        directory = os.path.expanduser(directory)\n        for target_class in sorted(class_to_idx.keys()):\n            class_index = class_to_idx[target_class]\n            target_dir = os.path.join(directory, target_class)\n            if not os.path.isdir(target_dir):\n                continue\n\n            for root, _, fnames in sorted(os.walk(target_dir)):\n                for fname in sorted(fnames):\n                    path = os.path.join(root, fname)\n                    if self.is_valid_image(path):\n                        item = (path, class_index)\n                        instances.append(item)\n        return instances\n\n    def is_valid_image(self, path):\n        try:\n            img = Image.open(path)\n            img.verify()  # PIL does not fully read the image file in verify\n            return True\n        except (IOError, SyntaxError, UnidentifiedImageError):\n            return False\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        img = Image.open(path).convert('RGB')\n        if self.transform is not None:\n            img = self.transform(img)\n        return img, target\n\n# Prepare the dataset\ndataset_path = '/kaggle/input/diamos-plant-dataset/Pear/leaves'\ntransform = transforms.Compose([\n    transforms.Resize((32, 32)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\nfull_dataset = ValidImageFolder(root=dataset_path, transform=transform)\n\n# Split the dataset into training and validation sets\ntrain_size = int(0.8 * len(full_dataset))\nval_size = len(full_dataset) - train_size\ntrain_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n\n# Training and evaluation functions\ndef train(model, device, train_loader, optimizer, criterion, epoch):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for batch_idx, (data, target) in enumerate(tqdm(train_loader)):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        outputs = model(data)\n        loss = criterion(outputs, target)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += target.size(0)\n        correct += predicted.eq(target).sum().item()\n    print(f'Epoch {epoch}: Loss: {running_loss/len(train_loader)}, Accuracy: {100.*correct/total}')\n\ndef validate(model, device, val_loader, criterion):\n    model.eval()\n    val_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in val_loader:\n            data, target = data.to(device), target.to(device)\n            outputs = model(data)\n            val_loss += criterion(outputs, target).item()\n            _, predicted = outputs.max(1)\n            correct += predicted.eq(target).sum().item()\n    val_loss /= len(val_loader.dataset)\n    print(f'Validation set: Average loss: {val_loss:.4f}, Accuracy: {correct}/{len(val_loader.dataset)} ({100.*correct/len(val_loader.dataset):.0f}%)')\n\n# Initialize and train the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = ExampleCNN(num_classes=len(full_dataset.classes)).to(device)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(1, 11):\n    train(model, device, train_loader, optimizer, criterion, epoch)\n    validate(model, device, val_loader, criterion)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T08:29:02.928371Z","iopub.execute_input":"2024-07-01T08:29:02.928704Z","iopub.status.idle":"2024-07-01T09:13:42.170983Z","shell.execute_reply.started":"2024-07-01T08:29:02.928679Z","shell.execute_reply":"2024-07-01T09:13:42.169928Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"100%|██████████| 38/38 [03:47<00:00,  5.98s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Loss: 0.7226115590647647, Accuracy: 71.21464226289517\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation set: Average loss: 0.0108, Accuracy: 445/602 (74%)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [03:32<00:00,  5.58s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Loss: 0.5367435860006433, Accuracy: 76.12312811980033\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation set: Average loss: 0.0107, Accuracy: 440/602 (73%)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [03:26<00:00,  5.43s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Loss: 0.49330240252770874, Accuracy: 77.99500831946756\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation set: Average loss: 0.0093, Accuracy: 464/602 (77%)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [03:34<00:00,  5.64s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Loss: 0.4164703833429437, Accuracy: 81.40599001663894\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation set: Average loss: 0.0100, Accuracy: 455/602 (76%)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [03:25<00:00,  5.41s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Loss: 0.40564152284672383, Accuracy: 82.32113144758735\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation set: Average loss: 0.0099, Accuracy: 459/602 (76%)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [03:28<00:00,  5.48s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Loss: 0.3648875971373759, Accuracy: 84.56738768718802\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation set: Average loss: 0.0093, Accuracy: 468/602 (78%)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [03:41<00:00,  5.84s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Loss: 0.34727511123607036, Accuracy: 84.69217970049917\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation set: Average loss: 0.0098, Accuracy: 469/602 (78%)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [03:23<00:00,  5.36s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Loss: 0.3191070650753222, Accuracy: 86.27287853577371\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation set: Average loss: 0.0091, Accuracy: 477/602 (79%)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [03:28<00:00,  5.48s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Loss: 0.2768731681924117, Accuracy: 88.64392678868552\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation set: Average loss: 0.0100, Accuracy: 472/602 (78%)\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 38/38 [03:25<00:00,  5.41s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Loss: 0.254689075444874, Accuracy: 89.76705490848586\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Validation set: Average loss: 0.0089, Accuracy: 482/602 (80%)\n","output_type":"stream"}]}]}