{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8367030,"sourceType":"datasetVersion","datasetId":4973674}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\n# Define the dataset path\ndataset_path = '/kaggle/input/diamos-plant-dataset/Pear/leaves'\n\n# Check if the dataset path exists\nif not os.path.exists(dataset_path):\n    print(f\"Error: Dataset path '{dataset_path}' does not exist.\")\nelse:\n    # Count the number of files in each subfolder (Pear and leaves)\n    for folder in os.listdir(dataset_path):\n        folder_path = os.path.join(dataset_path, folder)\n        if os.path.isdir(folder_path):\n            num_files = len(os.listdir(folder_path))\n            print(f\"Number of files in '{folder}' folder: {num_files}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-01T12:46:14.931934Z","iopub.execute_input":"2024-07-01T12:46:14.932283Z","iopub.status.idle":"2024-07-01T12:46:14.946829Z","shell.execute_reply.started":"2024-07-01T12:46:14.932255Z","shell.execute_reply":"2024-07-01T12:46:14.945948Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Number of files in 'curl' folder: 65\nNumber of files in 'healthy' folder: 43\nNumber of files in 'spot' folder: 1768\nNumber of files in 'slug' folder: 4050\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:42:43.154636Z","iopub.execute_input":"2024-07-01T12:42:43.155523Z","iopub.status.idle":"2024-07-01T12:42:47.925042Z","shell.execute_reply.started":"2024-07-01T12:42:43.155489Z","shell.execute_reply":"2024-07-01T12:42:47.924272Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Define the path to your dataset\ndataset_path = '/kaggle/input/diamos-plant-dataset/Pear/leaves'","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:42:56.420846Z","iopub.execute_input":"2024-07-01T12:42:56.421752Z","iopub.status.idle":"2024-07-01T12:42:56.427025Z","shell.execute_reply.started":"2024-07-01T12:42:56.421716Z","shell.execute_reply":"2024-07-01T12:42:56.426040Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Define transformations for preprocessing\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize image to 224x224\n    transforms.ToTensor(),  # Convert image to PyTorch tensor\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize image\n])","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:43:04.822497Z","iopub.execute_input":"2024-07-01T12:43:04.823135Z","iopub.status.idle":"2024-07-01T12:43:04.828162Z","shell.execute_reply.started":"2024-07-01T12:43:04.823104Z","shell.execute_reply":"2024-07-01T12:43:04.827199Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Function to check if image is valid\ndef is_valid_image(image_path):\n    try:\n        # Attempt to open and validate the image\n        Image.open(image_path).convert('RGB')\n        return True\n    except (FileNotFoundError, OSError, ValueError):\n        return False","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:43:17.412248Z","iopub.execute_input":"2024-07-01T12:43:17.412971Z","iopub.status.idle":"2024-07-01T12:43:17.417635Z","shell.execute_reply.started":"2024-07-01T12:43:17.412942Z","shell.execute_reply":"2024-07-01T12:43:17.416643Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Custom dataset class with error handling\nclass CustomImageFolder(ImageFolder):\n    def __init__(self, root, transform=None):\n        super().__init__(root, transform=transform)\n\n    def __getitem__(self, index):\n        # Get image path and label from original ImageFolder\n        path, target = self.samples[index]\n\n        # Check if image is valid\n        if not is_valid_image(path):\n            # Handle invalid image (e.g., return a placeholder image or skip)\n            # For simplicity, returning a placeholder image and an out-of-bounds target\n            invalid_image = torch.zeros((3, 224, 224))  # Placeholder image tensor\n            return invalid_image, len(self.classes)  # Return an out-of-bounds target\n\n        # Load and transform the image\n        image = self.loader(path)\n        if self.transform is not None:\n            image = self.transform(image)\n\n        return image, target","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:43:41.265824Z","iopub.execute_input":"2024-07-01T12:43:41.266548Z","iopub.status.idle":"2024-07-01T12:43:41.273047Z","shell.execute_reply.started":"2024-07-01T12:43:41.266506Z","shell.execute_reply":"2024-07-01T12:43:41.272105Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Load the custom dataset\ndataset = CustomImageFolder(dataset_path, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:44:16.751702Z","iopub.execute_input":"2024-07-01T12:44:16.752048Z","iopub.status.idle":"2024-07-01T12:44:17.744435Z","shell.execute_reply.started":"2024-07-01T12:44:16.752021Z","shell.execute_reply":"2024-07-01T12:44:17.743528Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Define the DataLoader\nbatch_size = 32\ndata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:44:18.683584Z","iopub.execute_input":"2024-07-01T12:44:18.683926Z","iopub.status.idle":"2024-07-01T12:44:18.688507Z","shell.execute_reply.started":"2024-07-01T12:44:18.683899Z","shell.execute_reply":"2024-07-01T12:44:18.687621Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Define the Channel Shuffle module\nclass ChannelShuffle(nn.Module):\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n\n    def forward(self, x):\n        batchsize, num_channels, height, width = x.data.size()\n        channels_per_group = num_channels // self.groups\n        # Reshape\n        x = x.view(batchsize, self.groups, channels_per_group, height, width)\n        # Transpose\n        x = torch.transpose(x, 1, 2).contiguous()\n        # Flatten\n        x = x.view(batchsize, -1, height, width)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:44:33.997922Z","iopub.execute_input":"2024-07-01T12:44:33.998732Z","iopub.status.idle":"2024-07-01T12:44:34.004873Z","shell.execute_reply.started":"2024-07-01T12:44:33.998699Z","shell.execute_reply":"2024-07-01T12:44:34.003937Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Define the RCSA module\nclass RCSA(nn.Module):\n    def __init__(self, in_channels, reduction=16, groups=4):\n        super(RCSA, self).__init__()\n        self.groups = groups\n        self.channel_shuffle = ChannelShuffle(groups)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv1 = nn.Conv2d(in_channels, in_channels // reduction, 1, bias=False)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(in_channels // reduction, in_channels, 1, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        residual = x\n        # Channel Shuffle\n        x = self.channel_shuffle(x)\n        # Squeeze and Excitation\n        x = self.avg_pool(x)\n        x = self.conv1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.sigmoid(x)\n        # Scale\n        x = residual * x\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:44:45.761132Z","iopub.execute_input":"2024-07-01T12:44:45.761451Z","iopub.status.idle":"2024-07-01T12:44:45.769328Z","shell.execute_reply.started":"2024-07-01T12:44:45.761426Z","shell.execute_reply":"2024-07-01T12:44:45.768340Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Define the CAM module\nclass CAM(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super(CAM, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, in_channels // reduction, kernel_size=1, bias=False)\n        self.conv2 = nn.Conv2d(in_channels // reduction, in_channels, kernel_size=1, bias=False)\n        self.sigmoid = nn.Sigmoid()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n    def forward(self, x):\n        avg_out = self.conv2(self.conv1(self.avg_pool(x)))\n        max_out = self.conv2(self.conv1(self.max_pool(x)))\n        out = avg_out + max_out\n        return x * self.sigmoid(out)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:44:55.958754Z","iopub.execute_input":"2024-07-01T12:44:55.959103Z","iopub.status.idle":"2024-07-01T12:44:55.966272Z","shell.execute_reply.started":"2024-07-01T12:44:55.959075Z","shell.execute_reply":"2024-07-01T12:44:55.965442Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Define your custom CNN architecture with RCSA and CAM\nclass CustomCNN(nn.Module):\n    def __init__(self, num_classes):\n        super(CustomCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n        self.rcsa1 = RCSA(16)\n        self.cam1 = CAM(16)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n        self.rcsa2 = RCSA(32)\n        self.cam2 = CAM(32)\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(32 * 56 * 56, 128)  # Adjust input size based on your image dimensions after pooling\n        self.fc2 = nn.Linear(128, num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.rcsa1(x)\n        x = self.cam1(x)\n        x = self.pool1(x)\n        x = self.relu(self.conv2(x))\n        x = self.rcsa2(x)\n        x = self.cam2(x)\n        x = self.pool2(x)\n        x = x.view(-1, 32 * 56 * 56)  # Adjust based on your image dimensions after pooling\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:45:11.462612Z","iopub.execute_input":"2024-07-01T12:45:11.462934Z","iopub.status.idle":"2024-07-01T12:45:11.475076Z","shell.execute_reply.started":"2024-07-01T12:45:11.462910Z","shell.execute_reply":"2024-07-01T12:45:11.474277Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Initialize the model\nmodel = CustomCNN(num_classes=len(dataset.classes))","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:45:21.106357Z","iopub.execute_input":"2024-07-01T12:45:21.106742Z","iopub.status.idle":"2024-07-01T12:45:21.257104Z","shell.execute_reply.started":"2024-07-01T12:45:21.106709Z","shell.execute_reply":"2024-07-01T12:45:21.256346Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss(ignore_index=len(dataset.classes))  # Ignore the out-of-bounds target\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:45:31.230737Z","iopub.execute_input":"2024-07-01T12:45:31.231078Z","iopub.status.idle":"2024-07-01T12:45:31.236616Z","shell.execute_reply.started":"2024-07-01T12:45:31.231049Z","shell.execute_reply":"2024-07-01T12:45:31.235646Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Function to calculate accuracy\ndef calculate_accuracy(outputs, labels):\n    _, predicted = torch.max(outputs, 1)\n    correct = (predicted == labels).sum().item()\n    total = labels.size(0)\n    accuracy = correct / total * 100\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-07-01T12:45:41.969428Z","iopub.execute_input":"2024-07-01T12:45:41.969818Z","iopub.status.idle":"2024-07-01T12:45:41.975252Z","shell.execute_reply.started":"2024-07-01T12:45:41.969790Z","shell.execute_reply":"2024-07-01T12:45:41.974216Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Train the model\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    total_correct = 0\n    total_samples = 0\n    for i, (images, labels) in enumerate(data_loader):\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n        # Calculate accuracy\n        _, predicted = torch.max(outputs, 1)\n        total_samples += labels.size(0)\n        total_correct += (predicted == labels).sum().item()\n\n        if (i+1) % 10 == 0:\n            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(data_loader)}], Loss: {running_loss / 10:.4f}')\n            running_loss = 0.0\n\n    # Print accuracy after each epoch\n    epoch_accuracy = total_correct / total_samples * 100\n    print(f'Epoch [{epoch+1}/{num_epochs}], Accuracy: {epoch_accuracy:.2f}%')\n\nprint('Finished Training.')\n\n# Save the trained model if needed\ntorch.save(model.state_dict(), 'custom_cnn_with_rcsa_cam.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}